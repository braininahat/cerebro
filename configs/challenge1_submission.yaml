# Lightning CLI configuration for Challenge 1 Final Submission
# Usage: uv run cerebro fit --config configs/challenge1_submission.yaml
#
# This config is for final submission (Day 10):
# - Trains on ALL subjects from training releases (R1-R4, R6-R11)
# - No validation split (maxim izes training data)
# - Tests on R5 (competition evaluation)
# - Architecture and hyperparameters should be locked before using this config

# Seed everything for reproducibility
seed_everything: 42

# Tuning flags (disabled for submission - hyperparameters should be locked)
run_lr_finder: false
run_batch_size_finder: false

# Trainer configuration
trainer:
  max_epochs: 1000
  accelerator: auto  # Auto-detect GPU/CPU
  devices: 1
  precision: "bf16-mixed" # Options: "32" (full), "16-mixed" (fast), "bf16-mixed" (stable+fast, requires Ampere+)
  gradient_clip_val: 1.0
  deterministic: true
  log_every_n_steps: 10

  # Logger configuration
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: eeg2025
      entity: ubcse-eeg2025  # Change to your wandb team/username
      name: challenge1_submission
      tags:
        - challenge1
        - supervised
        - eegnex
        - submission
        - final
      notes: "Final submission: Train on ALL subjects from training releases, test on R5"
      log_model: all  # Upload all checkpoints as wandb artifacts
      save_dir: outputs/challenge1

  # Callbacks
  callbacks:
    # ModelCheckpoint: Save models periodically (no validation to monitor in submission mode)
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: outputs/challenge1/checkpoints
        filename: challenge1-submission-{epoch:02d}
        save_top_k: 3
        save_last: true
        every_n_epochs: 10  # Save every 10 epochs since no validation

    # TQDMProgressBar: Keep progress bars after epoch completion
    - class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        leave: true

    # RichModelSummary: Display model architecture
    - class_path: lightning.pytorch.callbacks.RichModelSummary
      init_args:
        max_depth: 1

    # ModelAutopsy: Comprehensive diagnostics on training end
    - class_path: cerebro.callbacks.ModelAutopsyCallback
      init_args:
        run_on_training_end: true
        run_on_early_stop: false  # No early stopping in submission mode
        diagnostics:
          - predictions
          - gradients
          - activations
          - integrated_gradients  # Captum: IG attribution analysis (memory optimized)
          - layer_gradcam         # Captum: Layer-wise attention (memory optimized)
          - failure_modes
        save_plots: true
        log_to_wandb: true
        generate_report: true
        upload_report: true           # Upload markdown report to wandb (default: true)
        upload_summary_table: true    # Upload metrics table for cross-run comparison (default: true)
        upload_raw_artifacts: true    # ENABLED for submission: archive full attribution data
        num_samples: 500
        top_k_failures: 100
        ig_n_steps: 50
        ig_baseline: zero

# Model configuration (class registered in CLI, just specify __init__ params)
model:
  n_chans: 129  # HBN dataset channel count
  n_outputs: 1  # Regression output (RT)
  n_times: 200  # 2 seconds at 100 Hz
  sfreq: 100
  model_class: EEGNeX  # Architecture choice: "EEGNeX" or "SignalJEPA_PreLocal"
  model_kwargs: {}  # Model-specific hyperparameters (empty for EEGNeX defaults)
  lr: 0.001  # Should be tuned in dev mode before submission
  weight_decay: 0.00001  # Should be tuned in dev mode before submission
  epochs: 1000  # Must match trainer.max_epochs for scheduler
  input_scale: 1000.0  # Input scaling: 1.0 (volt, baseline), 1000.0 (millivolt, faster convergence)

# Data configuration (class registered in CLI, just specify __init__ params)
data:
  data_dir: ${oc.env:HBN_ROOT,data}
  # WARNING: R5 is the COMPETITION VALIDATION SET - NEVER include in training releases!
  # Training on ALL subjects from R1-R4, R6-R11 (10 releases total)
  releases:
    - R1
    - R2
    - R3
    - R4
    - R6
    - R7
    - R8
    - R9
    - R10
    - R11
  batch_size: 512
  num_workers: 8  # Parallel data loading (adjust based on CPU cores)
  excluded_subjects:
    - NDARWV769JM7
    - NDARME789TD2
    - NDARUA442ZVF
    - NDARJP304NK1
    - NDARTY128YLU
    - NDARDW550GU6
    - NDARLD243KRE
    - NDARUJ292JXV
    - NDARBA381JGH
  shift_after_stim: 0.5  # Start window 0.5s after stimulus
  window_len: 2.0  # 2 second windows
  use_mini: false  # Must be false for final submission
  cache_dir: null  # Deprecated/ignored by Challenge1DataModule
  val_frac: 0.1  # Ignored in submission mode
  test_frac: 0.1  # Ignored in submission mode
  seed: 2025
  sfreq: 100
  epoch_len_s: 2.0
  anchor: stimulus_anchor
  # Submission mode settings
  mode: submission  # "submission" = train on ALL subjects, no val split
  test_on_r5: true  # REQUIRED in submission mode
  r5_data_dir: ${oc.env:HBN_ROOT,data}  # R5 data location
