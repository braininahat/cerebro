# S-JEPA fine-tuning with PreLocal architecture (full fine-tuning) - Challenge 2
#
# PreLocal architecture (paper's best):
#   Spatial conv (129 → n_spat_filters) → Local encoder → FC → output
#
# Challenge 2: P_factor prediction from multi-task EEG
#
# Usage:
#   uv run cerebro fit --config configs/sjepa/finetune_challenge2_prelocal_full.yaml

seed_everything: 42
run_lr_finder: false
run_batch_size_finder: false

trainer:
  accelerator: auto
  devices: 1
  precision: bf16-mixed
  gradient_clip_val: 1.0
  max_epochs: 100
  log_every_n_steps: 5

  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: eeg2025
      entity: ubcse-eeg2025
      name: sjepa_finetune_c2_prelocal_full
      tags:
        - sjepa
        - finetune
        - challenge2
        - prelocal
        - full_finetuning
        - unfrozen_encoder
      save_dir: outputs/sjepa/finetune_c2_prelocal_full/${now:%Y%m%d_%H%M%S}
      mode: offline

  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: outputs/sjepa/finetune_c2_prelocal_full/${now:%Y%m%d_%H%M%S}/checkpoints
        filename: finetune-{epoch:02d}-{val/loss:.4f}
        monitor: val/loss
        mode: min
        save_top_k: 3
        save_last: true

    - class_path: cerebro.callbacks.LatestCheckpointSymlinkCallback

    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val/loss
        patience: 50  # Paper uses 50 for fine-tuning
        mode: min
        verbose: true

    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: epoch

    - class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        leave: true

model:
  class_path: cerebro.trainers.sjepa_finetune_prelocal.PreLocalFinetuneTrainer
  init_args:
    # Uses latest checkpoint from corresponding pretraining run.
    # Override via CLI if needed:
    # --model.pretrained_checkpoint="path/to/specific/checkpoint.ckpt"
    pretrained_checkpoint: outputs/sjepa/pretrain_full_80pct/latest/checkpoints/last.ckpt

    # Architecture params
    n_outputs: 1  # Regression (single output for p_factor)
    n_spat_filters: 4  # Number of virtual channels

    # Freezing strategy
    freeze_encoder: false  # Enable full fine-tuning
    warmup_epochs: 10

    # Optimizer params
    lr: 0.001
    encoder_lr_multiplier: 0.1
    weight_decay: 0.0001
    scheduler_patience: 10
    scheduler_factor: 0.5

    # Loss function
    loss_fn: mse  # MSE for p_factor regression

data:
  class_path: cerebro.data.base.BaseTaskDataModule
  init_args:
    dataset:
      class_path: cerebro.data.datasets.hbn.HBNDataset
      init_args:
        data_dir: ${oc.env:EEG2025_DATA_ROOT,./data}
        # Training releases (all except R5)
        releases:
          - R1
          - R2
          - R3
          - R4
          # R5 excluded (competition validation set)
          - R6
          - R7
          - R8
          - R9
          - R10
          - R11
        # Challenge 2 uses all tasks (multi-task aggregation)
        tasks:
          - contrastChangeDetection
          - restingState
          - despicableMe
          - thePresent
          - diaryOfAWimpyKid
          - funwithFractals
          - surroundSupp
        use_mini: false  # Full dataset

    task:
      class_path: cerebro.data.tasks.challenge2.Challenge2Task
      init_args:
        window_size_s: 2.0
        window_stride_s: 1.0
        sfreq: 100

    # Data splits and loading (MATCHES PRETRAINING)
    batch_size: 64
    num_workers: 4
    val_frac: 0.2  # 80:20 train:val split (matches pretrain)
    test_frac: 0.0  # No test split (R5 is external)
    seed: 42  # Same seed as pretrain for split reproducibility

    # Excluded subjects (known bad data)
    excluded_subjects:
      - NDARWV769JM7
      - NDARME789TD2
      - NDARUA442ZVF
      - NDARJP304NK1
      - NDARTY128YLU
      - NDARDW550GU6
      - NDARLD243KRE
      - NDARUJ292JXV
      - NDARBA381JGH
