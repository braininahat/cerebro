# S-JEPA fine-tuning with CUSTOM PreLocal architecture (with proper pretrained weight transfer)
#
# Custom PreLocal architecture (FIXES the braindecode limitation):
#   Spatial conv (129→n_spat_filters) → Local encoder → FC → output
#
# Our custom implementation properly transfers pretrained weights:
#   - Spatial conv: NEW (trained from scratch)
#   - Feature encoder first conv: NEW (dimension mismatch)
#   - Feature encoder other layers: PRETRAINED (transferred!)
#   - Classification head: NEW (trained from scratch)
#
# This gives us ~90% of the pretrained benefits, matching the paper's approach.
#
# Per paper (Figure 5, Page 5):
#   "16s-60% × full-pre-local occupies rank 1 in two-thirds of cases"
#   This custom implementation should achieve similar results.
#
# Usage:
#   # 1. First run pretraining (if not done already)
#   uv run cerebro fit --config configs/sjepa/pretrain_full_80pct.yaml
#
#   # 2. Then run fine-tuning with our CUSTOM implementation
#   uv run cerebro fit --config configs/sjepa/finetune_challenge1_prelocal_custom.yaml \
#     --model.pretrained_checkpoint="outputs/sjepa/pretrain_full_80pct/latest/checkpoints/last.ckpt"

seed_everything: 42
run_lr_finder: false
run_batch_size_finder: false

trainer:
  accelerator: auto
  devices: 1
  precision: bf16-mixed
  gradient_clip_val: 1.0
  max_epochs: 100
  log_every_n_steps: 5

  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: eeg2025
      entity: ubcse-eeg2025
      name: sjepa_finetune_c1_prelocal_custom
      tags:
        - sjepa
        - finetune
        - challenge1
        - prelocal
        - custom_implementation
        - proper_weight_transfer
        - full_finetuning
      save_dir: outputs/sjepa/finetune_c1_prelocal_custom/${now:%Y%m%d_%H%M%S}
      mode: offline

  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: outputs/sjepa/finetune_c1_prelocal_custom/${now:%Y%m%d_%H%M%S}/checkpoints
        filename: finetune-{epoch:02d}-{val/loss:.4f}
        monitor: val/loss
        mode: min
        save_top_k: 3
        save_last: true

    - class_path: cerebro.callbacks.LatestCheckpointSymlinkCallback

    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val/loss
        patience: 50  # Paper uses 50 for fine-tuning
        mode: min
        verbose: true

    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: epoch

    - class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        leave: true

model:
  class_path: cerebro.trainers.sjepa_finetune_prelocal_custom.CustomPreLocalFinetuneTrainer
  init_args:
    # Uses latest checkpoint from corresponding pretraining run.
    # Override via CLI if needed:
    # --model.pretrained_checkpoint="path/to/specific/checkpoint.ckpt"
    pretrained_checkpoint: outputs/sjepa/pretrain_full_80pct/latest/checkpoints/last.ckpt

    # Architecture params
    n_outputs: 1  # Regression (single output for response time)
    n_spat_filters: 4  # Number of virtual channels (paper tests 4, 8, 16)

    # Freezing strategy (PAPER'S BEST APPROACH)
    freeze_encoder: true  # Enable full fine-tuning (\"full-\" strategy)
    warmup_epochs: 10  # Paper uses 10-epoch warmup before unfreezing

    # Optimizer params
    lr: 0.001  # Learning rate for new layers
    encoder_lr_multiplier: 0.1  # Encoder gets 10x lower LR (0.0001) when unfrozen

    # LR finder at warmup transition (data-driven optimization)
    warmup_reruns_lr_finder: false  # Set to true to auto-tune LR when encoder unfreezes
    lr_finder_min: 1.0e-8  # Minimum LR for search range
    lr_finder_max: 1.0e-2  # Maximum LR for search range
    lr_finder_num_training: 100  # Number of training steps for LR finder

    weight_decay: 0.0001
    scheduler_patience: 10
    scheduler_factor: 0.5

    # Loss function
    loss_fn: mse  # MSE for response time regression

data:
  class_path: cerebro.data.jepa_pretrain.JEPAPretrainDataModule
  init_args:
    data_dir: ${oc.env:HBN_ROOT,./data}

    # Training releases (all except R5)
    releases:
      - R1
      - R2
      - R3
      - R4
      # R5 excluded (competition validation set)
      - R6
      - R7
      - R8
      - R9
      - R10
      - R11

    # Challenge 1: Only contrastChangeDetection task
    all_tasks:
      - contrastChangeDetection

    # Challenge 1: Event-based windowing with trial-level targets
    task_windowing:
      class_path: cerebro.data.tasks.challenge1.Challenge1Task
      init_args:
        window_len: 2.0
        shift_after_stim: 0.5
        sfreq: 100
        epoch_len_s: 2.0
        anchor: stimulus_anchor

    # Data splits and loading (MATCHES PRETRAINING)
    batch_size: 512
    num_workers: 16
    val_split: 0.2  # 80:20 train:val split (matches pretrain)
    test_release: "R5"  # Competition validation set
    seed: 42  # Same seed as pretrain for split reproducibility
    mini: false  # Full dataset
    persistent_workers: true
    prefetch_factor: 1