# Contrastive pretraining configuration
# @package training

name: contrastive

# Optimizer
optimizer:
  name: AdamW
  lr: 0.001
  weight_decay: 0.0001  # Higher weight decay for pretraining
  betas: [0.9, 0.999]

# Learning rate scheduler with warmup
scheduler:
  name: CosineAnnealingWarmRestarts
  T_0: 10  # Restart every 10 epochs
  T_mult: 2
  eta_min: 0.00001

warmup:
  enabled: true
  epochs: 5
  start_lr: 0.00001

# Training parameters
epochs: 50
early_stopping:
  enabled: true
  patience: 15
  min_delta: 0.001
  monitor: val_loss
  mode: min

# Contrastive loss
loss:
  name: InfoNCE
  temperature: ${model.temperature}  # 0.07

# Negative sampling
num_negatives: 256  # Number of negative samples per positive

# Validation
val_every_n_epochs: 1
save_top_k: 3

# Gradient clipping
gradient_clip_val: 1.0

# Mixed precision
use_amp: true

# Logging
log_every_n_steps: 10
