# LaBraM MEM Pretraining - Full HBN Dataset
#
# Phase 2 of LaBraM: Masked EEG Modeling (MEM) pretraining
# Requires pretrained tokenizer from Phase 1 (codebook_full.yaml)
#
# Usage: uv run cerebro fit --config configs/labram/pretrain_full.yaml
#
# Training time: ~48-72 hours on single GPU
# Output: Pretrained LaBraM encoder checkpoint

seed_everything: 42

# Tuning flags
run_lr_finder: false
run_batch_size_finder: false

# LR finder parameters
lr_finder_min: 1.0e-07
lr_finder_max: 0.1
lr_finder_num_training: 100

# Batch size finder parameters
bs_finder_mode: power
bs_finder_init_val: 64
bs_finder_max_trials: 6
bs_finder_steps_per_trial: 3

model:
  class_path: "cerebro.models.labram.pretrain.MEMPretrainModule"

  init_args:
    # Pretrained tokenizer weights (UPDATE THIS PATH!)
    pretrained: true
    pretrained_weight: outputs/labram/codebook_full/checkpoints/last.ckpt

    # Input configuration
    EEG_size: 200  # 2 seconds @ 100Hz
    patch_size: 100  # 1 second patches
    in_chans: 1
    out_chans: 8

    # MEM Student Model architecture
    vocab_size: 8192  # Must match codebook n_code
    embed_dim: 200    # Student model embedding dimension
    depth: 12         # Transformer depth
    num_heads: 10     # Attention heads
    mlp_ratio: 4.0
    qkv_bias: true
    drop_rate: 0.0
    attn_drop_rate: 0.0
    drop_path_rate: 0.0
    init_std: 0.02

    # VQNSP Tokenizer parameters (must match Phase 1)
    n_code: 8192
    code_dim: 32
    encoder_depth: 24
    decoder_depth: 3
    decay: 0.99

    # Training parameters
    learning_rate: 0.0001
    weight_decay: 0.05
    betas: [0.9, 0.95]
    mask_ratio: 0.5  # Mask 50% of tokens
    scale_eeg: false
    chunk_pad: true
    use_cosine_per_step: true
    eta_min: 0.000001

    # Channel and temporal hints
    n_chans: 129
    max_time_window_hint: 2

# Trainer configuration
trainer:
  max_epochs: 300  # Full pretraining
  accelerator: auto
  devices: 1
  precision: "bf16-mixed"
  gradient_clip_val: 1.0
  deterministic: true
  log_every_n_steps: 50

  # Logger configuration
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: eeg2025
      entity: ubcse-eeg2025
      name: labram_pretrain_full
      tags:
        - labram
        - pretrain
        - mem
        - full
      notes: "LaBraM MEM pretraining on full HBN dataset"
      save_dir: outputs/labram/pretrain_full
      mode: offline

  # Callbacks
  callbacks:
    - class_path: cerebro.callbacks.CheckpointCompatibilityCallback

    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: outputs/labram/pretrain_full/checkpoints
        filename: pretrain-{epoch:02d}-{val_mem_loss:.4f}
        monitor: val_mem_loss
        mode: min
        save_top_k: 3
        save_last: true

    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_mem_loss
        patience: 30
        mode: min
        verbose: true

    - class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        leave: true

    - class_path: lightning.pytorch.callbacks.RichModelSummary
      init_args:
        max_depth: 2

    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: epoch

# Data configuration
data:
  data_dir: ${oc.env:HBN_ROOT,data}

  # Full HBN dataset (R5 excluded - competition validation!)
  releases:
    - R1
    - R2
    - R3
    - R4
    - R6
    - R7
    - R8
    - R9
    - R10
    - R11

  data_req: pretrain

  # Passive tasks for self-supervised learning
  passive_tasks:
    - restingState
    - surroundSupp
    - despicableMe
    - thePresent
    - diaryOfAWimpyKid
    - funwithFractals

  active_task: contrastChangeDetection

  # DataLoader parameters
  batch_size: 128
  num_workers: 16
  window_len_s: 2.0
  stride_s: 1.0
  sfreq: 100
  val_split: 0.1
  seed: 2025
