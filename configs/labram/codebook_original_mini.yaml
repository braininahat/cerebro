# LaBraM Codebook Training - Original-Matched Mini
# Phase 1: VQ-NSP Tokenizer (matches original LaBraM base architecture)
# Dataset: R1-R4, R6-R11 mini (20 subjects per release, ~120 total)
# Hardware: Single GPU with gradient accumulation (effective batch 512)
seed_everything: 42
run_lr_finder: false
run_batch_size_finder: false

model:
  class_path: "cerebro.models.labram.tokenizer.VQNSP"
  init_args:
    pretrained: false
    as_tokenizer: false
    pretrained_weight: null
    # Codebook parameters (original)
    n_code: 8192
    code_dim: 32
    decay: 0.99
    # Architecture parameters (original base model)
    EEG_size: 200  # 2s @ 100Hz (vs original 1600 = 8s @ 200Hz)
    patch_size: 100  # 1s @ 100Hz (vs original 200 = 1s @ 200Hz)
    encoder_depth: 12  # CHANGED from 24 to match original base
    decoder_depth: 3
    # HBN adaptations
    n_chans: 129  # HBN has 129 channels (vs original 62)
    max_time_window_hint: 2
    # Training hyperparameters (original)
    learning_rate: 0.0002  # Base 5e-5 scaled for batch 512 (64×8)
    weight_decay: 0.0001  # CHANGED from 0.05 to match original
    warmup_epochs: 5  # ADDED: original warmup
    min_lr: 0.00001  # ADDED: original min LR
    # Normalization (CRITICAL!)
    scale_eeg: true  # CHANGED: divide by 100 like original

trainer:
  max_epochs: 100  # CHANGED from 2 to match original
  accelerator: auto
  devices: 1
  precision: "bf16-mixed"
  gradient_clip_val: null  # REMOVED: original has no clipping
  accumulate_grad_batches: 8  # ADDED: effective batch 512 (matches 8 GPUs × 64)
  deterministic: true
  log_every_n_steps: 10

  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: eeg2025
      entity: ubcse-eeg2025
      name: labram_codebook_original_mini
      tags: [labram, codebook, original, mini, phase1]
      save_dir: outputs/labram/codebook_original_mini
      mode: offline

  callbacks:
    - class_path: cerebro.callbacks.CheckpointCompatibilityCallback
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: outputs/labram/codebook_original_mini/checkpoints
        filename: tokenizer-{epoch:02d}-{val_tok_loss:.4f}
        monitor: val_tok_loss
        mode: min
        save_top_k: 3
        save_last: true
    - class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        leave: true

data:
  class_path: cerebro.data.labram_pretrain.LaBraMPretrainDataModule
  init_args:
    data_dir: ${oc.env:HBN_ROOT,data}
    releases: [R1, R2, R3, R4, R6, R7, R8, R9, R10, R11]  # CHANGED: all mini releases except R5
    use_mini: true  # 20 subjects per release
    passive_tasks: [RestingState, DespicableMe]
    batch_size: 64  # Original per-GPU batch size
    num_workers: 4
    window_len: 2.0
    patch_size: 100
    sfreq: 100
    val_frac: 0.2  # CHANGED: use ALL data for pretraining (no val needed for dynamics check)
    test_frac: 0.2
    seed: 2025
