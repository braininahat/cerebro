# LaBraM Codebook Training - Mini (R1, 2 epochs)
# Quick validation of Phase 1: Tokenizer training pipeline
seed_everything: 42
run_lr_finder: false
run_batch_size_finder: false

model:
  class_path: "cerebro.models.labram.tokenizer.VQNSP"
  init_args:
    pretrained: false
    as_tokenizer: false
    pretrained_weight: null
    n_code: 8192
    code_dim: 32
    EEG_size: 200
    patch_size: 100
    encoder_depth: 24
    decoder_depth: 3
    decay: 0.99
    learning_rate: 0.0001
    weight_decay: 0.05
    scale_eeg: false
    n_chans: 129
    max_time_window_hint: 2

trainer:
  max_epochs: 2  # Mini: just 2 epochs
  accelerator: auto
  devices: 1
  precision: "bf16-mixed"
  gradient_clip_val: 1.0
  deterministic: true
  log_every_n_steps: 10

  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: eeg2025
      entity: ubcse-eeg2025
      name: labram_codebook_mini
      tags: [labram, codebook, mini, test]
      save_dir: outputs/labram/codebook_mini
      mode: offline

  callbacks:
    - class_path: cerebro.callbacks.CheckpointCompatibilityCallback
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: outputs/labram/codebook_mini/checkpoints
        filename: tokenizer-{epoch:02d}-{val_tok_loss:.4f}
        monitor: val_tok_loss
        mode: min
        save_top_k: 1
        save_last: true
    - class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        leave: true

data:
  class_path: cerebro.data.labram_pretrain.LaBraMPretrainDataModule
  init_args:
    data_dir: ${oc.env:EEG2025_DATA_ROOT,data}
    releases: [R1]  # Mini: R1 only
    passive_tasks: [RestingState, DespicableMe]  # Capitalize task names
    batch_size: 64
    num_workers: 4
    window_len: 2.0
    patch_size: 100
    sfreq: 100
    val_frac: 0.1
    seed: 2025
