# ============================================================================
# TUH EEG Tokenizer Training Configuration
# ============================================================================
# Trains VQNSP (Vector-Quantized Neural Signal Processing) tokenizer on TUH dataset
#
# Usage:
#   1. Set environment variable for HDF5 file path:
#      export TUH_HDF5_PATH=/path/to/tuh_eeg_virtual.h5
#
#   2. Run training:
#      uv run cerebro fit --config configs/labram/codebook_tuh.yaml
#
#   3. With custom batch size:
#      uv run cerebro fit --config configs/labram/codebook_tuh.yaml \
#          --data.init_args.batch_size 256
#
# Prerequisites:
#   - TUH data processed to HDF5 format (see scripts/process_tuh_to_sharded_hdf5.py)
#   - Virtual HDF5 file created (see scripts/create_virtual_hdf5.py)
#
# Output:
#   - Checkpoints: outputs/checkpoints/tokenizer-tuh/
#   - Logs: outputs/logs/tokenizer-tuh/
#   - WandB: project "tuh-eeg-tokenizer"
# ============================================================================

# Seed everything for reproducibility
seed_everything: 42

# Tuning flags (root level, not under trainer)
run_lr_finder: false
run_batch_size_finder: false

# LR finder parameters (used if run_lr_finder=true)
lr_finder_min: 1.0e-07
lr_finder_max: 0.1
lr_finder_num_training: 100

# Batch size finder parameters (used if run_batch_size_finder=true)
bs_finder_mode: power
bs_finder_init_val: 64
bs_finder_max_trials: 6
bs_finder_steps_per_trial: 3

model:
  class_path: "cerebro.models.labram.tokenizer.VQNSP"

  init_args:
    # Model architecture
    pretrained: false
    as_tokenizer: false
    pretrained_weight: null # Set to checkpoint path, e.g., "checkpoints/codebook/model.pt"

    # Codebook parameters
    n_code: 8192 # CODEBOOK_SIZE
    code_dim: 32 # CODEBOOK_EMD_DIM

    # Input configuration
    EEG_size: 200 # INPUT_SIZE (2 seconds @ 100Hz)
    patch_size: 100 # 1 second patches @ 100Hz
    encoder_depth: 24 # ENCODER_DEPTH
    decoder_depth: 3 # DECODER_DEPTH
    # EMA quantizer parameters
    decay: 0.99 # EMA_DECAY
    learning_rate: 0.0001
    weight_decay: 0.05
    scale_eeg: false # Whether to scale EEG inputs (like the original Labram engine)

    # Channel and temporal hints
    # Note: TUH has variable channel counts depending on montage
    # 01_tcp_ar montage typically has 20-22 channels
    # This will be inferred from data, but hint for model initialization
    n_chans: null # Will be inferred from TUH data
    max_time_window_hint: 2 # Maximum time window in seconds

# Trainer configuration
trainer:
  max_epochs: 1000
  accelerator: auto # Auto-detect GPU/CPU
  devices: 1
  precision: "32-true" # Options: "32" (full), "16-mixed" (fast), "bf16-mixed" (stable+fast, requires Ampere+)
  gradient_clip_val: 1.0
  deterministic: true
  log_every_n_steps: 10

  # Logger configuration
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: tuh-eeg-tokenizer
      entity: ubcse-eeg2025 # Change to your wandb team/username
      name: tokenizer-tuh
      tags:
        - tokenizer
        - tuh
      notes: "Training Tokenizer on TUH EEG Dataset"
      save_dir: outputs/logs/tokenizer-tuh
      mode: online # "online" or "offline" or "disabled" (for debugging without internet)

  # Callbacks
  callbacks:
    # CheckpointCompatibilityCallback: Fix checkpoints for resuming (must be FIRST)
    - class_path: cerebro.callbacks.CheckpointCompatibilityCallback

    # ModelCheckpoint: Save best models based on validation loss
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: outputs/checkpoints/tokenizer-tuh
        filename: tokenizer-tuh-{epoch:02d}-{val_tok_loss:.4f}
        monitor: val_tok_loss
        mode: min
        save_top_k: 3
        save_last: true

    # EarlyStopping: Stop training if validation loss doesn't improve
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_tok_loss
        patience: 20
        mode: min
        verbose: true

    # TQDMProgressBar: Keep progress bars after epoch completion
    - class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        leave: true

    # RichModelSummary: Display model architecture
    - class_path: lightning.pytorch.callbacks.RichModelSummary
      init_args:
        max_depth: 1

data:
  class_path: cerebro.data.tuh.TUHDataModule
  init_args:
    # HDF5 file path (virtual or sharded)
    hdf5_path: ${oc.env:TUH_HDF5_PATH,data/tuh/tuh_eeg_virtual.h5}

    # Target variable (null for unsupervised tokenizer training)
    target_name: null

    # DataLoader parameters
    batch_size: 512
    num_workers: 16

    # Train/val/test splits (subject-level)
    train_ratio: 0.8
    val_ratio: 0.1
    test_ratio: 0.1
    seed: 42

    # Optional filters
    recording_ids: null
    subjects: null
    sessions: null
    montages: null  # Could filter to specific montage, e.g., ['01_tcp_ar']
