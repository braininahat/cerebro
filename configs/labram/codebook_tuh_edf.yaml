# ============================================================================
# TUH EEG Tokenizer Training Configuration (EDF Backend)
# ============================================================================
# Trains VQNSP tokenizer on TUH dataset loaded directly from EDF files
# with preprocessing and window caching.
#
# Advantages over HDF5 approach:
# - No preprocessing step required
# - Original data stays intact (1.7TB EDF files)
# - Flexible preprocessing (can change filters/resampling on the fly)
# - Cached windows reused across runs with same parameters
#
# Usage:
#   1. Set environment variable for TUH directory:
#      export TUH_DIR=/path/to/tuh/tueg/v2.0.1
#
#   2. First run (creates cache):
#      uv run cerebro fit --config configs/labram/codebook_tuh_edf.yaml
#
#   3. Subsequent runs (uses cache):
#      uv run cerebro fit --config configs/labram/codebook_tuh_edf.yaml
#
#   4. Change preprocessing (creates new cache):
#      uv run cerebro fit --config configs/labram/codebook_tuh_edf.yaml \
#          --data.init_args.sfreq 200 \
#          --data.init_args.window_size_s 4.0
#
# Cache behavior:
#   - Windows cached at: {TUH_DIR}/cache/windows_*.pkl
#   - Cache key includes all preprocessing parameters
#   - Change any parameter â†’ new cache created
#   - Delete cache to reprocess: rm {TUH_DIR}/cache/windows_*.pkl
#
# Output:
#   - Checkpoints: outputs/checkpoints/tokenizer-tuh-edf/
#   - Logs: outputs/logs/tokenizer-tuh-edf/
#   - WandB: project "tuh-eeg-tokenizer"
# ============================================================================

# Seed everything for reproducibility
seed_everything: 42

# Tuning flags
run_lr_finder: false
run_batch_size_finder: true

# LR finder parameters
lr_finder_min: 1.0e-07
lr_finder_max: 0.1
lr_finder_num_training: 100

# Batch size finder parameters
bs_finder_mode: power
bs_finder_init_val: 64
bs_finder_max_trials: 6
bs_finder_steps_per_trial: 3

model:
  class_path: "cerebro.models.labram.tokenizer.VQNSP"

  init_args:
    # Model architecture
    pretrained: false
    as_tokenizer: false
    pretrained_weight: null

    # Codebook parameters
    n_code: 8192 # CODEBOOK_SIZE
    code_dim: 32 # CODEBOOK_EMD_DIM

    # Input configuration
    EEG_size: 200 # INPUT_SIZE (2 seconds @ 100Hz)
    patch_size: 100 # 1 second patches @ 100Hz
    encoder_depth: 24 # ENCODER_DEPTH
    decoder_depth: 3 # DECODER_DEPTH

    # EMA quantizer parameters
    decay: 0.99 # EMA_DECAY
    learning_rate: 0.0001
    weight_decay: 0.05
    scale_eeg: false

    # Channel and temporal hints
    # TUH channels are standardized to 21 common 10-20 EEG channels
    n_chans: 21 # Standardized channel count (automatic channel selection)
    max_time_window_hint: 2

# Trainer configuration
trainer:
  max_epochs: 1000
  accelerator: auto
  devices: 1
  precision: "32-true"
  gradient_clip_val: 1.0
  deterministic: true
  log_every_n_steps: 10

  # Logger configuration
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: eeg-challenge-2025
      entity: ubcse-eeg2025
      name: tokenizer-tuh-edf
      tags:
        - tokenizer
        - tuh
        - edf
      notes: "Training Tokenizer on TUH EEG Dataset (EDF backend with caching)"
      save_dir: outputs/logs/tokenizer-tuh-edf
      mode: online

  # Callbacks
  callbacks:
    # CheckpointCompatibilityCallback: Fix checkpoints for resuming (must be FIRST)
    - class_path: cerebro.callbacks.CheckpointCompatibilityCallback

    # ModelCheckpoint: Save best models based on validation loss
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: outputs/checkpoints/tokenizer-tuh-edf
        filename: tokenizer-tuh-edf-{epoch:02d}-{val_tok_loss:.4f}
        monitor: val_tok_loss
        mode: min
        save_top_k: 3
        save_last: true

    # EarlyStopping: Stop training if validation loss doesn't improve
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_tok_loss
        patience: 20
        mode: min
        verbose: true

    # TQDMProgressBar: Keep progress bars after epoch completion
    - class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        leave: true

    # RichModelSummary: Display model architecture
    - class_path: lightning.pytorch.callbacks.RichModelSummary
      init_args:
        max_depth: 1

data:
  class_path: cerebro.data.tuh_edf.TUHEDFDataModule
  init_args:
    # TUH directory (containing edf/ subdirectory)
    tuh_dir: ${oc.env:TUH_DIR,/projects/academic/wenyaoxu/anarghya/research/eeg-data/tuh/tueg/v2.0.1}

    # Target variable (null for unsupervised tokenizer training)
    target_name: null

    # Dataset filters
    montages: ["01_tcp_ar"] # Use consistent montage
    # Note: Channels are automatically standardized to 21 common 10-20 EEG channels
    # (FP1, FP2, F7, F3, FZ, F4, F8, A1, T3, C3, CZ, C4, T4, A2, T5, P3, PZ, P4, T6, O1, O2)

    # DataLoader parameters
    batch_size: 512
    num_workers: 16

    # Train/val/test splits (subject-level)
    train_ratio: 0.8
    val_ratio: 0.1
    test_ratio: 0.1
    seed: 42

    # Optional filters
    recording_ids: null
    subjects: null
    sessions: null
    max_recordings: null # Set to small number (e.g., 100) for testing

    # Windowing parameters (matching HBN preprocessing)
    window_size_s: 4.0 # 4-second windows (matching HBN)
    window_stride_s: 2.0 # 2-second stride = 2s overlap (matching HBN)
    crop_size_s: 2.0 # Final 2-second crops from 4s windows (matching HBN)

    # Preprocessing parameters (matching HBN)
    sfreq: 100.0 # Resample to 100Hz (matching HBN)
    apply_bandpass: true # Apply bandpass filter (0.5-49 Hz)
    l_freq: 0.5 # Low frequency cutoff (matching HBN)
    h_freq: 50.0 # High frequency cutoff (must be < Nyquist = sfreq/2 = 50 Hz)
    min_recording_duration_s: 4.0 # Filter out recordings < 4s

    # Caching parameters
    cache_dir: null # Auto: {tuh_dir}/cache/
    use_cache: true # Use cached windows if available
    filelist_cache: data/tuh/filelist.txt # Cache file list for fast directory scanning (~100-300x faster)
