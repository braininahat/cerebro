betas: !!python/tuple
- 0.9
- 0.95
cache_dir: null
decay: 0.99
decoder_config:
  EEG_size: 2.0
  attn_drop_rate: 0.0
  depth: 3
  drop_path_rate: 0.0
  drop_rate: 0.0
  embed_dim: 200
  in_chans: 64
  init_scale: 0.001
  init_values: 0.0
  mlp_ratio: 4.0
  norm_layer: !!python/object/apply:functools.partial
    args:
    - &id001 !!python/name:torch.nn.modules.normalization.LayerNorm ''
    state: !!python/tuple
    - *id001
    - !!python/tuple []
    - eps: 1.0e-06
    - null
  num_classes: 0
  num_heads: 10
  patch_size: 1
  qk_scale: null
  qkv_bias: true
  use_abs_pos_emb: true
  use_mean_pooling: true
  use_rel_pos_bias: false
  use_shared_rel_pos_bias: false
decoder_out_dim: 100
embed_dim: 64
encoder_config:
  EEG_size: 200.0
  attn_drop_rate: 0.0
  depth: 12
  drop_path_rate: 0.0
  drop_rate: 0.0
  embed_dim: 200
  in_chans: 1
  init_scale: 0.001
  init_values: 0.0
  mlp_ratio: 4.0
  norm_layer: !!python/object/apply:functools.partial
    args:
    - *id001
    state: !!python/tuple
    - *id001
    - !!python/tuple []
    - eps: 1.0e-06
    - null
  num_classes: 0
  num_heads: 10
  patch_size: 100
  qk_scale: null
  qkv_bias: true
  use_abs_pos_emb: true
  use_mean_pooling: true
  use_rel_pos_bias: false
  use_shared_rel_pos_bias: false
learning_rate: 0.0001
log_lr: true
n_chans_hint: null
n_embed: 8192
patch_len: 100
pretrained_cfg: null
pretrained_cfg_overlay: null
quantize_kmeans_init: true
smooth_l1_loss: false
weight_decay: 0.05
